# EcoDrive Query API

API REST desenvolvida em Python para processar queries de clientes da EcoDrive, convertendo o flow do Dify em um servi√ßo standalone.

## üìã √çndice

- [Vis√£o Geral](#vis√£o-geral)
- [Arquitetura](#arquitetura)
- [Funcionalidades](#funcionalidades)
- [Tecnologias](#tecnologias)
- [Pr√©-requisitos](#pr√©-requisitos)
- [Instala√ß√£o](#instala√ß√£o)
- [Configura√ß√£o](#configura√ß√£o)
- [Uso](#uso)
- [Endpoints](#endpoints)
- [Fluxo de Processamento](#fluxo-de-processamento)
- [Docker](#docker)
- [Desenvolvimento](#desenvolvimento)
- [Integra√ß√£o com Knowledge Base](#integra√ß√£o-com-knowledge-base)
- [Alternativa com Langflow](#alternativa-com-langflow)
- [Troubleshooting](#troubleshooting)

## üéØ Vis√£o Geral

Esta API converte o workflow do Dify da EcoDrive em um servi√ßo Python independente com FastAPI. O sistema processa queries de clientes, classifica inten√ß√µes e fornece respostas contextualizadas atrav√©s de diferentes modelos LLM.

**Fluxo Original (Dify):**
```
Input (phone + query) ‚Üí User Lookup ‚Üí Intent Classification ‚Üí LLM Routing ‚Üí Response
```

**Implementa√ß√£o (FastAPI):**
```python
POST /query
{
  "query": "Quanto custa o patinete X2?",
  "phone": "+56912345678"
}

‚Üí Response
{
  "answer": "Resposta personalizada...",
  "intent": "informacoes",
  "conversation_id": "uuid"
}
```

## üèóÔ∏è Arquitetura

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                      EcoDrive Query API                      ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                               ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ
‚îÇ  ‚îÇ   FastAPI    ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ   External API Service       ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  (main.py)   ‚îÇ      ‚îÇ  - User lookup by phone      ‚îÇ    ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îÇ  - Chatwoot notifications    ‚îÇ    ‚îÇ
‚îÇ         ‚îÇ              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ
‚îÇ         ‚îÇ                                                   ‚îÇ
‚îÇ         ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ
‚îÇ         ‚îÇ              ‚îÇ   LLM Service (OpenAI)       ‚îÇ    ‚îÇ
‚îÇ         ‚îÇ              ‚îÇ  - Intent classification     ‚îÇ    ‚îÇ
‚îÇ         ‚îÇ              ‚îÇ  - Greeting responses        ‚îÇ    ‚îÇ
‚îÇ         ‚îÇ              ‚îÇ  - Customer service          ‚îÇ    ‚îÇ
‚îÇ         ‚îÇ              ‚îÇ  - Query improvement         ‚îÇ    ‚îÇ
‚îÇ         ‚îÇ              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ
‚îÇ         ‚îÇ                                                   ‚îÇ
‚îÇ         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ
‚îÇ                        ‚îÇ   RAG Service                ‚îÇ    ‚îÇ
‚îÇ                        ‚îÇ  - Context retrieval         ‚îÇ    ‚îÇ
‚îÇ                        ‚îÇ  - Knowledge base search     ‚îÇ    ‚îÇ
‚îÇ                        ‚îÇ  - Response generation       ‚îÇ    ‚îÇ
‚îÇ                        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ
‚îÇ                                                               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ                          ‚îÇ                  ‚îÇ
         ‚ñº                          ‚ñº                  ‚ñº
  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
  ‚îÇ External    ‚îÇ          ‚îÇ   OpenAI    ‚îÇ    ‚îÇ   Cohere    ‚îÇ
  ‚îÇ RAG API     ‚îÇ          ‚îÇ     API     ‚îÇ    ‚îÇ (Reranking) ‚îÇ
  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## ‚ú® Funcionalidades

### Classifica√ß√£o de Inten√ß√£o
O sistema classifica automaticamente a query do usu√°rio em:
- **saudacao**: Cumprimentos e sauda√ß√µes
- **informacoes**: Perguntas sobre produtos e informa√ß√µes
- **atendimento**: Solicita√ß√µes de atendimento humano
- **reclamacao**: Reclama√ß√µes e problemas
- **elogio**: Elogios e feedback positivo
- **outros**: Queries fora do escopo

### Roteamento Inteligente
Baseado na inten√ß√£o classificada, a query √© roteada para:
- **LLM de Sauda√ß√£o**: Respostas calorosas e apresenta√ß√£o
- **LLM RAG**: Busca em knowledge base para informa√ß√µes
- **LLM de Atendimento**: Encaminhamento para humanos
- **LLM de Elogio**: Agradecimentos
- **LLM Outros**: Redirecionamento educado

### Contexto de Conversa
- Mant√©m hist√≥rico de conversa√ß√£o
- Considera contexto em respostas
- Suporta m√∫ltiplas conversas simult√¢neas

### Integra√ß√µes Externas
- Busca de dados de usu√°rio por telefone
- Notifica√ß√µes para Chatwoot
- Knowledge base retrieval (RAG)

## üõ†Ô∏è Tecnologias

- **FastAPI**: Framework web moderno e r√°pido
- **Pydantic**: Valida√ß√£o de dados e settings
- **OpenAI**: Modelos LLM (GPT-3.5-turbo, o3-mini)
- **Cohere**: Reranking de resultados (opcional)
- **HTTPX**: Cliente HTTP ass√≠ncrono
- **Uvicorn**: Servidor ASGI
- **Docker**: Containeriza√ß√£o

## üìã Pr√©-requisitos

- Python 3.11+
- Docker e Docker Compose (opcional)
- OpenAI API Key
- Cohere API Key (opcional, para reranking)
- Acesso √† API externa RAG (configurada no Dify)

## üöÄ Instala√ß√£o

### Op√ß√£o 1: Instala√ß√£o Local

```bash
# Clone o reposit√≥rio
git clone https://github.com/AIMILV1-commits/Difytest.git
cd Difytest

# Crie um ambiente virtual
python -m venv venv
source venv/bin/activate  # Linux/Mac
# ou
venv\Scripts\activate  # Windows

# Instale as depend√™ncias
pip install -r requirements.txt
```

### Op√ß√£o 2: Docker (Recomendado)

```bash
# Clone o reposit√≥rio
git clone https://github.com/AIMILV1-commits/Difytest.git
cd Difytest

# Configure as vari√°veis de ambiente (veja pr√≥xima se√ß√£o)
cp .env.example .env
# Edite .env com suas credenciais

# Build e execute com Docker Compose
docker-compose up -d
```

## ‚öôÔ∏è Configura√ß√£o

### 1. Configurar Vari√°veis de Ambiente

```bash
cp .env.example .env
```

Edite o arquivo `.env` e configure:

**Obrigat√≥rias:**
```env
# OpenAI
OPENAI_API_KEY=sk-your-key-here

# External RAG API
RAG_API_BASE_URL=https://rag-api.rpaclick.com
RAG_API_KEY=your-base64-encoded-key

# Knowledge Base Dataset IDs
DATASET_IDS=dataset1,dataset2,dataset3
```

**Opcionais:**
```env
# Cohere (para reranking)
COHERE_API_KEY=your-cohere-key

# Redis (para produ√ß√£o)
REDIS_URL=redis://localhost:6379/0
```

### 2. Configurar Knowledge Base

‚ö†Ô∏è **IMPORTANTE**: A implementa√ß√£o atual do RAG Service √© um placeholder. Para habilitar a busca real em knowledge base, voc√™ precisa:

1. Escolher um vector database:
   - **Pinecone**: Managed, escal√°vel
   - **Weaviate**: Open source, self-hosted
   - **ChromaDB**: Lightweight, local
   - **Qdrant**: High performance

2. Implementar o m√©todo `retrieve_context` em `app/services/rag_service.py`:

```python
async def retrieve_context(self, query: str, top_k: int = 5) -> str:
    # 1. Gerar embedding da query
    embedding = await self.openai_client.embeddings.create(
        model="text-embedding-3-small",
        input=query
    )

    # 2. Buscar no vector store
    results = await your_vector_db.search(
        vector=embedding.data[0].embedding,
        top_k=top_k
    )

    # 3. Opcional: Rerank com Cohere
    if self.cohere_client:
        reranked = await self.cohere_client.rerank(
            query=query,
            documents=[r.text for r in results],
            model=settings.COHERE_RERANK_MODEL
        )
        results = reranked.results

    # 4. Formatar contexto
    return "\n\n".join([r.text for r in results])
```

## üìñ Uso

### Iniciar o Servidor

**Local:**
```bash
# Desenvolvimento (com auto-reload)
uvicorn app.main:app --reload --host 0.0.0.0 --port 8000

# Produ√ß√£o
uvicorn app.main:app --host 0.0.0.0 --port 8000 --workers 4
```

**Docker:**
```bash
docker-compose up -d
```

### Testar a API

**Health Check:**
```bash
curl http://localhost:8000/health
```

**Enviar Query:**
```bash
curl -X POST http://localhost:8000/query \
  -H "Content-Type: application/json" \
  -d '{
    "query": "Hola, buenos d√≠as!",
    "phone": "+56912345678"
  }'
```

**Resposta:**
```json
{
  "answer": "¬°Hola! üòä Soy Rodrigo de EcoDrive. ¬øC√≥mo puedo ayudarte hoy?",
  "intent": "saudacao",
  "conversation_id": "abc123-def456-ghi789"
}
```

## üîå Endpoints

### `GET /`
Health check b√°sico
```json
{
  "status": "healthy",
  "version": "1.0.0"
}
```

### `GET /health`
Health check detalhado
```json
{
  "status": "healthy",
  "version": "1.0.0"
}
```

### `POST /query`
Processa uma query do usu√°rio

**Request:**
```json
{
  "query": "Quanto custa o patinete X2?",
  "phone": "+56912345678",
  "conversation_id": "optional-uuid"  // Opcional
}
```

**Response:**
```json
{
  "answer": "El patinete X2 cuesta $299.990 CLP...",
  "intent": "informacoes",
  "conversation_id": "abc123-def456"
}
```

### `DELETE /conversation/{conversation_id}`
Deleta hist√≥rico de conversa

```bash
curl -X DELETE http://localhost:8000/conversation/abc123
```

## üîÑ Fluxo de Processamento

```mermaid
graph TD
    A[POST /query] --> B[Buscar Usu√°rio por Telefone]
    B --> C[Classificar Inten√ß√£o]
    C --> D{Qual Inten√ß√£o?}

    D -->|saudacao| E[LLM Sauda√ß√£o]
    D -->|informacoes| F[Melhorar Query]
    D -->|atendimento| G[LLM Atendimento]
    D -->|reclamacao| G
    D -->|elogio| H[LLM Elogio]
    D -->|outros| I[LLM Outros]

    F --> J[Buscar Knowledge Base]
    J --> K[LLM RAG]

    E --> L[Retornar Resposta]
    K --> L
    G --> M[Notificar Chatwoot]
    M --> L
    H --> L
    I --> L

    L --> N[Atualizar Hist√≥rico]
```

## üê≥ Docker

### Build da Imagem
```bash
docker build -t ecodrive-api .
```

### Executar Container
```bash
docker run -d \
  --name ecodrive-api \
  -p 8000:8000 \
  --env-file .env \
  ecodrive-api
```

### Docker Compose
```bash
# Iniciar
docker-compose up -d

# Parar
docker-compose down

# Ver logs
docker-compose logs -f api

# Rebuild
docker-compose up -d --build
```

### Com Redis (opcional)
Descomente a se√ß√£o Redis no `docker-compose.yml`:

```yaml
redis:
  image: redis:7-alpine
  container_name: ecodrive-redis
  ports:
    - "6379:6379"
  volumes:
    - redis-data:/data
  restart: unless-stopped
  networks:
    - ecodrive-network
```

E configure no `.env`:
```env
REDIS_URL=redis://redis:6379/0
```

## üíª Desenvolvimento

### Estrutura do Projeto
```
Difytest/
‚îú‚îÄ‚îÄ app/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ main.py                 # FastAPI app principal
‚îÇ   ‚îú‚îÄ‚îÄ config.py               # Configura√ß√µes
‚îÇ   ‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ schemas.py          # Modelos Pydantic
‚îÇ   ‚îî‚îÄ‚îÄ services/
‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îú‚îÄ‚îÄ external_api.py     # Chamadas API externas
‚îÇ       ‚îú‚îÄ‚îÄ llm_service.py      # Servi√ßo OpenAI
‚îÇ       ‚îî‚îÄ‚îÄ rag_service.py      # Servi√ßo RAG
‚îú‚îÄ‚îÄ Dockerfile
‚îú‚îÄ‚îÄ docker-compose.yml
‚îú‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ .env.example
‚îú‚îÄ‚îÄ README.md
‚îî‚îÄ‚îÄ Ecodrive 1.0 Prod.yml      # Flow original do Dify
```

### Adicionar Nova Inten√ß√£o

1. **Adicionar ao schema** (`app/models/schemas.py`):
```python
class IntentClassification(BaseModel):
    intent: Literal["saudacao", "informacoes", "atendimento",
                    "reclamacao", "elogio", "outros", "nova_intencao"]
```

2. **Atualizar classificador** (`app/services/llm_service.py`):
```python
# Adicionar √† classification_rules no prompt do classify_intent
```

3. **Implementar handler** (`app/services/llm_service.py`):
```python
async def generate_nova_intencao_response(self, query: str, history: List[Dict] = None) -> str:
    # Implementa√ß√£o
    pass
```

4. **Adicionar roteamento** (`app/main.py`):
```python
elif intent == "nova_intencao":
    answer = await llm_service.generate_nova_intencao_response(
        request.query,
        history
    )
```

### Executar Testes

```bash
# Instalar depend√™ncias de teste
pip install pytest pytest-asyncio httpx

# Executar testes
pytest tests/

# Com cobertura
pytest --cov=app tests/
```

## üß© Integra√ß√£o com Knowledge Base

### Op√ß√µes de Vector Database

#### 1. Pinecone (Managed)
```python
import pinecone

pinecone.init(api_key="your-key", environment="us-west1-gcp")
index = pinecone.Index("ecodrive-kb")

# Buscar
results = index.query(
    vector=embedding,
    top_k=5,
    include_metadata=True
)
```

#### 2. Weaviate (Open Source)
```python
import weaviate

client = weaviate.Client("http://localhost:8080")

# Buscar
results = client.query.get("Document", ["content"]) \
    .with_near_vector({"vector": embedding}) \
    .with_limit(5) \
    .do()
```

#### 3. ChromaDB (Local)
```python
import chromadb

client = chromadb.Client()
collection = client.get_collection("ecodrive-kb")

# Buscar
results = collection.query(
    query_embeddings=[embedding],
    n_results=5
)
```

### Exemplo de Implementa√ß√£o Completa

Veja o arquivo `app/services/rag_service.py` para implementar a integra√ß√£o completa com seu vector database de escolha.

## üåä Alternativa com Langflow

Se preferir usar **Langflow** (conforme sugerido), voc√™ pode:

### 1. Instalar Langflow
```bash
pip install langflow
```

### 2. Importar o Flow do Dify
```bash
langflow run
# Acesse http://localhost:7860
# Importe o arquivo "Ecodrive 1.0 Prod.yml"
```

### 3. Exportar como API
Langflow permite exportar flows como APIs REST automaticamente, similar ao que implementamos aqui.

### Vantagens do Langflow:
- ‚úÖ Interface visual para edi√ß√£o
- ‚úÖ Deploy mais r√°pido
- ‚úÖ Integra√ß√£o nativa com vector stores
- ‚úÖ Debugging visual

### Vantagens da Implementa√ß√£o Python:
- ‚úÖ Controle total do c√≥digo
- ‚úÖ Customiza√ß√£o completa
- ‚úÖ Melhor performance
- ‚úÖ Facilidade de testes
- ‚úÖ CI/CD mais simples

## üêõ Troubleshooting

### Erro: "OpenAI API Key not found"
```bash
# Verifique se a chave est√° configurada
echo $OPENAI_API_KEY

# Configure no .env
OPENAI_API_KEY=sk-your-key-here
```

### Erro: "Connection refused" para External API
```bash
# Verifique a URL base
curl https://rag-api.rpaclick.com/health

# Verifique o Bearer token
curl -H "Authorization: Bearer YOUR_TOKEN" \
  https://rag-api.rpaclick.com/api/v1/client/user-client/by-phone/123
```

### Knowledge Base n√£o retorna resultados
‚ö†Ô∏è Implemente a integra√ß√£o real com vector database em `app/services/rag_service.py`

### Docker build falha
```bash
# Limpar cache e rebuild
docker-compose down
docker system prune -a
docker-compose up --build
```

### Logs da Aplica√ß√£o
```bash
# Docker
docker-compose logs -f api

# Local
# Os logs aparecem no console onde voc√™ executou uvicorn
```

## üìù Licen√ßa

Este projeto √© parte do reposit√≥rio EcoDrive.

## ü§ù Contribuindo

1. Fork o projeto
2. Crie uma branch para sua feature (`git checkout -b feature/AmazingFeature`)
3. Commit suas mudan√ßas (`git commit -m 'Add some AmazingFeature'`)
4. Push para a branch (`git push origin feature/AmazingFeature`)
5. Abra um Pull Request

## üìß Contato

Para quest√µes e suporte, abra uma issue no reposit√≥rio.

---

**Nota**: Esta implementa√ß√£o converte o flow do Dify para Python standalone. Para uma solu√ß√£o visual e mais r√°pida, considere usar Langflow conforme sugerido.
